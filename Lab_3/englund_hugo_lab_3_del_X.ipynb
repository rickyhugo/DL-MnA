{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"englund_hugo_lab_3_del_X.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPp1HbYV0KFHE7MQJgS3aAT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1JxtEUNST2-b"},"source":["### Hugo Englund | 2020-11-27\n"]},{"cell_type":"markdown","metadata":{"id":"skZyr2HCUN9b"},"source":["# Laboration 3: Part X\n","## Outline\n","The aim of this labarotaion is to analyze the Fashion MNIST dataset by transfer learning, through the following steps:\n","1. Import the pretrained Xception model from Keras.\n","2. Scale the images to be applicable in the model.\n","3. Modify the layers in the model, as well as adding layers to adjust the model to the given classification problem.\n","4. Train and the test the modified Xception model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iIR6uCCvVPqB"},"source":["#### Import relevant packages"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IH569V9-Tv22","executionInfo":{"status":"ok","timestamp":1606423230658,"user_tz":-60,"elapsed":2448,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"eeddba77-9a51-4145-c0ee-98d57acda658"},"source":["# import relevant packages\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import cv2\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qKcK2_iibJUn"},"source":["#### (Performance plot function)"]},{"cell_type":"code","metadata":{"id":"c55_W0Y0bGnv"},"source":["# plots accuracy and loss for training and validation, respectively\n","def plot_performance(history):\n","  # plot accuracy during training and validation\n","  plt.plot(history.history[\"accuracy\"], label=\"Training\\naccuracy\")\n","  plt.plot(history.history[\"val_accuracy\"], label=\"Validation\\naccuracy\")\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Accuracy\")\n","  plt.ylim([0.5, 1])\n","  plt.legend(loc=\"best\")\n","  plt.title(\"Training and validation accuracies\")\n","  plt.grid(b=True)\n","  plt.show()\n","\n","  # plot loss during training and validation\n","  plt.plot(history.history[\"loss\"], label=\"Training\\nloss\")\n","  plt.plot(history.history[\"val_loss\"], label='Validation\\nloss')\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Loss\")\n","  plt.ylim([0, 0.5])\n","  plt.legend(loc=\"best\")\n","  plt.title(\"Training and validation losses\")\n","  plt.grid(b=True)\n","  plt.show()\n","  return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"su2QAsgHVVB9"},"source":["#### Import the Fashion MNIST data set"]},{"cell_type":"code","metadata":{"id":"q1HCWHd8VVaw"},"source":["# fetch data\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","\n","# Save class names since they are not included in the imported data set\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","# load data and split to train and test sets\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XkWsAlQQY_hJ"},"source":["#### Data preparation\n","Before the modelling, we prepare the data by:\n","1. Scaling the images to at least 32x32 pixels since it is the smallest dimension applicable in VGG16.\n","2. Adding another dimension to the images as the RGB-channel, required in the model.\n","3. Verifying that the image scaling went correctly by plotting some images and investigating their dimensions.\n","4. Normalizing the data to a range between 0 and 1, in order for the features in each image to be in a common scale.\n","5. Converting the labels to one-hot encoded vectors since we have multiple outputs in the model."]},{"cell_type":"code","metadata":{"id":"wierpjRkY-0i"},"source":["# define desired dimension of scaled images\n","# 71x71 is choosen since it is the smallest possible\n","DIM = (71, 71)\n","\n","# scale images to desired size and add RGB-channels, e.g. 71x71x3\n","train_images = np.array([cv2.cvtColor(cv2.resize(img, DIM), cv2.COLOR_GRAY2BGR) for img in train_images])\n","test_images = np.array([cv2.cvtColor(cv2.resize(img, DIM), cv2.COLOR_GRAY2BGR) for img in test_images])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6WZvGvmUJfM","executionInfo":{"status":"ok","timestamp":1606423233388,"user_tz":-60,"elapsed":5148,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"7c6a30b7-ba36-4718-c176-97e0327e6468"},"source":["# check dimension of train and test images\n","print(\"Dimension of scaled images:\")\n","print(\"Training images: {}\".format(train_images.shape))\n","print(\"Test images: {}\".format(test_images.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimension of scaled images:\n","Training images: (60000, 71, 71, 3)\n","Test images: (10000, 71, 71, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wcRjiokQagEG"},"source":["#### Normalize images and convert labels\n"]},{"cell_type":"code","metadata":{"id":"R6cePRq3wN0E"},"source":["from keras.utils.np_utils import to_categorical \n","# normalize image pixels\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","# convert labels to one-hot encoded vectors\n","train_labels, test_labels = to_categorical(train_labels, num_classes=10), to_categorical(test_labels, num_classes=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X9L0fGEVXPMw"},"source":["#### Import the pretrained Xception model\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lXZWGb-3XOO5","executionInfo":{"status":"ok","timestamp":1606423238699,"user_tz":-60,"elapsed":10444,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"656f4327-0a14-4e79-9b6b-d89c78416753"},"source":["# import pretrained model\n","xcep = tf.keras.applications.Xception(\n","    include_top=False,\n","    weights=\"imagenet\",\n","    input_shape=(DIM[0], DIM[0], 3),\n",")\n","\n","# make the imported layers nontrainable\n","xcep.trainable = False\n","\n","# summarize the model\n","xcep.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"xception\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 71, 71, 3)]  0                                            \n","__________________________________________________________________________________________________\n","block1_conv1 (Conv2D)           (None, 35, 35, 32)   864         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","block1_conv1_bn (BatchNormaliza (None, 35, 35, 32)   128         block1_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block1_conv1_act (Activation)   (None, 35, 35, 32)   0           block1_conv1_bn[0][0]            \n","__________________________________________________________________________________________________\n","block1_conv2 (Conv2D)           (None, 33, 33, 64)   18432       block1_conv1_act[0][0]           \n","__________________________________________________________________________________________________\n","block1_conv2_bn (BatchNormaliza (None, 33, 33, 64)   256         block1_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block1_conv2_act (Activation)   (None, 33, 33, 64)   0           block1_conv2_bn[0][0]            \n","__________________________________________________________________________________________________\n","block2_sepconv1 (SeparableConv2 (None, 33, 33, 128)  8768        block1_conv2_act[0][0]           \n","__________________________________________________________________________________________________\n","block2_sepconv1_bn (BatchNormal (None, 33, 33, 128)  512         block2_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block2_sepconv2_act (Activation (None, 33, 33, 128)  0           block2_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block2_sepconv2 (SeparableConv2 (None, 33, 33, 128)  17536       block2_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block2_sepconv2_bn (BatchNormal (None, 33, 33, 128)  512         block2_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 17, 17, 128)  8192        block1_conv2_act[0][0]           \n","__________________________________________________________________________________________________\n","block2_pool (MaxPooling2D)      (None, 17, 17, 128)  0           block2_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 17, 17, 128)  512         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 17, 17, 128)  0           block2_pool[0][0]                \n","                                                                 batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv1_act (Activation (None, 17, 17, 128)  0           add[0][0]                        \n","__________________________________________________________________________________________________\n","block3_sepconv1 (SeparableConv2 (None, 17, 17, 256)  33920       block3_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv1_bn (BatchNormal (None, 17, 17, 256)  1024        block3_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block3_sepconv2_act (Activation (None, 17, 17, 256)  0           block3_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block3_sepconv2 (SeparableConv2 (None, 17, 17, 256)  67840       block3_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv2_bn (BatchNormal (None, 17, 17, 256)  1024        block3_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 9, 9, 256)    32768       add[0][0]                        \n","__________________________________________________________________________________________________\n","block3_pool (MaxPooling2D)      (None, 9, 9, 256)    0           block3_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 9, 9, 256)    1024        conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 9, 9, 256)    0           block3_pool[0][0]                \n","                                                                 batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","block4_sepconv1_act (Activation (None, 9, 9, 256)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","block4_sepconv1 (SeparableConv2 (None, 9, 9, 728)    188672      block4_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block4_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block4_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block4_sepconv2_act (Activation (None, 9, 9, 728)    0           block4_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block4_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block4_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block4_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block4_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 5, 5, 728)    186368      add_1[0][0]                      \n","__________________________________________________________________________________________________\n","block4_pool (MaxPooling2D)      (None, 5, 5, 728)    0           block4_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 5, 5, 728)    2912        conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 5, 5, 728)    0           block4_pool[0][0]                \n","                                                                 batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","block5_sepconv1_act (Activation (None, 5, 5, 728)    0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","block5_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block5_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block5_sepconv2_act (Activation (None, 5, 5, 728)    0           block5_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block5_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block5_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block5_sepconv3_act (Activation (None, 5, 5, 728)    0           block5_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block5_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block5_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 5, 5, 728)    0           block5_sepconv3_bn[0][0]         \n","                                                                 add_2[0][0]                      \n","__________________________________________________________________________________________________\n","block6_sepconv1_act (Activation (None, 5, 5, 728)    0           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","block6_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block6_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block6_sepconv2_act (Activation (None, 5, 5, 728)    0           block6_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block6_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block6_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block6_sepconv3_act (Activation (None, 5, 5, 728)    0           block6_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block6_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block6_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 5, 5, 728)    0           block6_sepconv3_bn[0][0]         \n","                                                                 add_3[0][0]                      \n","__________________________________________________________________________________________________\n","block7_sepconv1_act (Activation (None, 5, 5, 728)    0           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","block7_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block7_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block7_sepconv2_act (Activation (None, 5, 5, 728)    0           block7_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block7_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block7_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block7_sepconv3_act (Activation (None, 5, 5, 728)    0           block7_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block7_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block7_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 5, 5, 728)    0           block7_sepconv3_bn[0][0]         \n","                                                                 add_4[0][0]                      \n","__________________________________________________________________________________________________\n","block8_sepconv1_act (Activation (None, 5, 5, 728)    0           add_5[0][0]                      \n","__________________________________________________________________________________________________\n","block8_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block8_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block8_sepconv2_act (Activation (None, 5, 5, 728)    0           block8_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block8_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block8_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block8_sepconv3_act (Activation (None, 5, 5, 728)    0           block8_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block8_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block8_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 5, 5, 728)    0           block8_sepconv3_bn[0][0]         \n","                                                                 add_5[0][0]                      \n","__________________________________________________________________________________________________\n","block9_sepconv1_act (Activation (None, 5, 5, 728)    0           add_6[0][0]                      \n","__________________________________________________________________________________________________\n","block9_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block9_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block9_sepconv2_act (Activation (None, 5, 5, 728)    0           block9_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block9_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block9_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block9_sepconv3_act (Activation (None, 5, 5, 728)    0           block9_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block9_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block9_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 5, 5, 728)    0           block9_sepconv3_bn[0][0]         \n","                                                                 add_6[0][0]                      \n","__________________________________________________________________________________________________\n","block10_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","block10_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block10_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block10_sepconv2_act (Activatio (None, 5, 5, 728)    0           block10_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block10_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","block10_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","block10_sepconv3_act (Activatio (None, 5, 5, 728)    0           block10_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","block10_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv3_act[0][0]       \n","__________________________________________________________________________________________________\n","block10_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv3[0][0]           \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 5, 5, 728)    0           block10_sepconv3_bn[0][0]        \n","                                                                 add_7[0][0]                      \n","__________________________________________________________________________________________________\n","block11_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_8[0][0]                      \n","__________________________________________________________________________________________________\n","block11_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block11_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block11_sepconv2_act (Activatio (None, 5, 5, 728)    0           block11_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block11_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","block11_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","block11_sepconv3_act (Activatio (None, 5, 5, 728)    0           block11_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","block11_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv3_act[0][0]       \n","__________________________________________________________________________________________________\n","block11_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv3[0][0]           \n","__________________________________________________________________________________________________\n","add_9 (Add)                     (None, 5, 5, 728)    0           block11_sepconv3_bn[0][0]        \n","                                                                 add_8[0][0]                      \n","__________________________________________________________________________________________________\n","block12_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_9[0][0]                      \n","__________________________________________________________________________________________________\n","block12_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block12_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block12_sepconv2_act (Activatio (None, 5, 5, 728)    0           block12_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block12_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","block12_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","block12_sepconv3_act (Activatio (None, 5, 5, 728)    0           block12_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","block12_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv3_act[0][0]       \n","__________________________________________________________________________________________________\n","block12_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv3[0][0]           \n","__________________________________________________________________________________________________\n","add_10 (Add)                    (None, 5, 5, 728)    0           block12_sepconv3_bn[0][0]        \n","                                                                 add_9[0][0]                      \n","__________________________________________________________________________________________________\n","block13_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_10[0][0]                     \n","__________________________________________________________________________________________________\n","block13_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block13_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block13_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block13_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block13_sepconv2_act (Activatio (None, 5, 5, 728)    0           block13_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block13_sepconv2 (SeparableConv (None, 5, 5, 1024)   752024      block13_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","block13_sepconv2_bn (BatchNorma (None, 5, 5, 1024)   4096        block13_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 3, 3, 1024)   745472      add_10[0][0]                     \n","__________________________________________________________________________________________________\n","block13_pool (MaxPooling2D)     (None, 3, 3, 1024)   0           block13_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 3, 3, 1024)   4096        conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","add_11 (Add)                    (None, 3, 3, 1024)   0           block13_pool[0][0]               \n","                                                                 batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","block14_sepconv1 (SeparableConv (None, 3, 3, 1536)   1582080     add_11[0][0]                     \n","__________________________________________________________________________________________________\n","block14_sepconv1_bn (BatchNorma (None, 3, 3, 1536)   6144        block14_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block14_sepconv1_act (Activatio (None, 3, 3, 1536)   0           block14_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block14_sepconv2 (SeparableConv (None, 3, 3, 2048)   3159552     block14_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block14_sepconv2_bn (BatchNorma (None, 3, 3, 2048)   8192        block14_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","block14_sepconv2_act (Activatio (None, 3, 3, 2048)   0           block14_sepconv2_bn[0][0]        \n","==================================================================================================\n","Total params: 20,861,480\n","Trainable params: 0\n","Non-trainable params: 20,861,480\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a_dchsTXppUk"},"source":["#### Find where the entry flow ends\n","It is necessary to remove some layers in the Xception in order to reduce the complexity of the model since our problem is considered \"easier\" than what Xception is designed for.\n","\n","As discussed previously, in part A and B, our images may not be that suitable for models pretrained on the ImageNet data since it differ considerably from our images. Therefore, only the entry flow is kept in the modified Xception model. I believe that keeping the middle flow in the model would overcomplicate the model in relation to our problem. Instead of improving the performance of the model, it would likely prolong training time without any payoff in accuracy. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GjVJIfxiASUQ","executionInfo":{"status":"ok","timestamp":1606423238700,"user_tz":-60,"elapsed":10435,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"a579e155-a53f-43e4-c05d-55928c7db105"},"source":["# find index for the first layer in the middle flow\n","# i.e., index fo the layer named \"block5_sepconv1_act\"\n","for idx in range(len(xcep.layers)):\n","    if xcep.layers[idx].name == 'block5_sepconv1_act':\n","        stopping_index = idx\n","        break\n","print(stopping_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["36\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xd4PsqFpO-hK"},"source":["#### Modify the Xception model \n","Modify the pretrained model by removing all existing layers beyond the entry flow, and adding additional dense layers, with regularization, suitable for our problem. More specifically:\n","\n","1. Pretrained Xception model with only the entry flow.\n","2. Additonal, trainable dense layers with ReLu-activation function and regularization corresponding to batch normalization and dropout of rate 0.5:\n","    - 4096 units\n","    - 4096 units\n","    - 2048 units\n","    - 1024 units\n","    - 516 units\n","    - 516 units\n","\n","3. Softmax activation with 10 units.\n","\n","This is done by Keras functional API in which we chain layer calls to create a modified model."]},{"cell_type":"code","metadata":{"id":"hByaR3p4O9mf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606423239025,"user_tz":-60,"elapsed":10751,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"cf05582e-b119-4826-ce8e-55a63b349583"},"source":["from tensorflow.keras import layers\n","from keras.models import Model\n","\n","# reduce the model by removing all layers beyond \"block5_sepconv1_act\"\n","xcep_reduced = Model(inputs=xcep.input, outputs=xcep.layers[-(len(xcep.layers) - stopping_index + 3)].output)\n","\n","# add custom layers to the reduced model\n","flatten = layers.Flatten()(xcep_reduced.output)\n","\n","# dense layer 1\n","x = layers.Dense(4096, activation='relu')(flatten)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.50)(x)\n","\n","# dense layer 2\n","x = layers.Dense(4096, activation='relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.50)(x)\n","\n","# dense layer 3\n","x = layers.Dense(2048, activation='relu')(x) \n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.50)(x)\n","\n","# dense layer 4\n","x = layers.Dense(1024, activation='relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.50)(x)\n","\n","# dense layer 5\n","x = layers.Dense(516, activation='relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.50)(x)\n","\n","# dense layer 6\n","x = layers.Dense(516, activation='relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.50)(x)\n","\n","# activation layer\n","out = layers.Dense(10, activation='softmax')(x)\n","\n","# build the modified model\n","xcep_mod = Model(inputs=xcep.input, outputs=out)\n","\n","xcep_mod.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 71, 71, 3)]  0                                            \n","__________________________________________________________________________________________________\n","block1_conv1 (Conv2D)           (None, 35, 35, 32)   864         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","block1_conv1_bn (BatchNormaliza (None, 35, 35, 32)   128         block1_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block1_conv1_act (Activation)   (None, 35, 35, 32)   0           block1_conv1_bn[0][0]            \n","__________________________________________________________________________________________________\n","block1_conv2 (Conv2D)           (None, 33, 33, 64)   18432       block1_conv1_act[0][0]           \n","__________________________________________________________________________________________________\n","block1_conv2_bn (BatchNormaliza (None, 33, 33, 64)   256         block1_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block1_conv2_act (Activation)   (None, 33, 33, 64)   0           block1_conv2_bn[0][0]            \n","__________________________________________________________________________________________________\n","block2_sepconv1 (SeparableConv2 (None, 33, 33, 128)  8768        block1_conv2_act[0][0]           \n","__________________________________________________________________________________________________\n","block2_sepconv1_bn (BatchNormal (None, 33, 33, 128)  512         block2_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block2_sepconv2_act (Activation (None, 33, 33, 128)  0           block2_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block2_sepconv2 (SeparableConv2 (None, 33, 33, 128)  17536       block2_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block2_sepconv2_bn (BatchNormal (None, 33, 33, 128)  512         block2_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 17, 17, 128)  8192        block1_conv2_act[0][0]           \n","__________________________________________________________________________________________________\n","block2_pool (MaxPooling2D)      (None, 17, 17, 128)  0           block2_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 17, 17, 128)  512         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 17, 17, 128)  0           block2_pool[0][0]                \n","                                                                 batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv1_act (Activation (None, 17, 17, 128)  0           add[0][0]                        \n","__________________________________________________________________________________________________\n","block3_sepconv1 (SeparableConv2 (None, 17, 17, 256)  33920       block3_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv1_bn (BatchNormal (None, 17, 17, 256)  1024        block3_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block3_sepconv2_act (Activation (None, 17, 17, 256)  0           block3_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block3_sepconv2 (SeparableConv2 (None, 17, 17, 256)  67840       block3_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv2_bn (BatchNormal (None, 17, 17, 256)  1024        block3_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 9, 9, 256)    32768       add[0][0]                        \n","__________________________________________________________________________________________________\n","block3_pool (MaxPooling2D)      (None, 9, 9, 256)    0           block3_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 9, 9, 256)    1024        conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 9, 9, 256)    0           block3_pool[0][0]                \n","                                                                 batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","block4_sepconv1_act (Activation (None, 9, 9, 256)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","block4_sepconv1 (SeparableConv2 (None, 9, 9, 728)    188672      block4_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block4_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block4_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block4_sepconv2_act (Activation (None, 9, 9, 728)    0           block4_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block4_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block4_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block4_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block4_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block4_pool (MaxPooling2D)      (None, 5, 5, 728)    0           block4_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 18200)        0           block4_pool[0][0]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 4096)         74551296    flatten[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 4096)         16384       dense[0][0]                      \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 4096)         0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 4096)         16781312    dropout[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 4096)         16384       dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 4096)         0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 2048)         8390656     dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 2048)         8192        dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 2048)         0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1024)         2098176     dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 1024)         4096        dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 1024)         0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 516)          528900      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 516)          2064        dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 516)          0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 516)          266772      dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 516)          2064        dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 516)          0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 10)           5170        dropout_5[0][0]                  \n","==================================================================================================\n","Total params: 103,595,810\n","Trainable params: 102,646,874\n","Non-trainable params: 948,936\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y7fNOeZoZJz2"},"source":["#### Compile the modified model\n","We use the Adam optimizer with a learning rate of 0.001."]},{"cell_type":"code","metadata":{"id":"ceNvildrZJLf"},"source":["from keras.optimizers import Adam\n","\n","# define optimizer\n","optimizer = Adam(learning_rate=0.001)\n","\n","# Compile the model \n","xcep_mod.compile(\n","    optimizer = optimizer,\n","    loss = \"categorical_crossentropy\", \n","    metrics=[\"accuracy\"]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7sAQmSkaARn"},"source":["#### Train the modified model\n","The validation split is set to the fraction corresponding to exactly 10000 images. Thus, we get an approximate 70`/`15`/`15%-split between training, testing and validation images, respectively. \n","\n","In addition, early stopping with a patience of two epochs is used in the model, which is that if the validation loss is no longer decreasing in four consecutive epochs, the training is terminated. This used to prevent excessive training times and foremost overfitting. The callback to reduction of the learning rate is also used in this model, which reduces the learning rate if the validation loss has not decreased between two epochs.\n","\n","We use 30 epochs and a batch size of 100."]},{"cell_type":"code","metadata":{"id":"jJv-CeGQZ_tI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606423370861,"user_tz":-60,"elapsed":142573,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"296ee812-8d83-41f8-9fab-85b8c5952326"},"source":["# define epochs and batch size\n","epochs = 30\n","batch_size = 100\n","\n","# callback: early stopping \n","early_stop = tf.keras.callbacks.EarlyStopping(patience=2, verbose=1)\n","\n","# callback: reduced learning rate on plateau\n","reduce_learning = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor=0.2,\n","    patience=1,\n","    verbose=1,\n","    mode='auto',\n","    min_delta=0.0001,\n","    cooldown=1,\n","    min_lr=0\n",")\n","\n","# fit the model\n","history = xcep_mod.fit(\n","    train_images,\n","    train_labels,\n","    batch_size=batch_size,\n","    validation_split=1/6,\n","    epochs=epochs,\n","    callbacks=[early_stop, reduce_learning]\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","500/500 [==============================] - 11s 22ms/step - loss: 0.7087 - accuracy: 0.7561 - val_loss: 0.2854 - val_accuracy: 0.8986\n","Epoch 2/30\n","500/500 [==============================] - 11s 21ms/step - loss: 0.3274 - accuracy: 0.8900 - val_loss: 0.2752 - val_accuracy: 0.9035\n","Epoch 3/30\n","500/500 [==============================] - 11s 21ms/step - loss: 0.2754 - accuracy: 0.9066 - val_loss: 0.2171 - val_accuracy: 0.9214\n","Epoch 4/30\n","499/500 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.9145\n","Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","500/500 [==============================] - 11s 21ms/step - loss: 0.2504 - accuracy: 0.9145 - val_loss: 0.2195 - val_accuracy: 0.9246\n","Epoch 5/30\n","500/500 [==============================] - 11s 21ms/step - loss: 0.1958 - accuracy: 0.9337 - val_loss: 0.1752 - val_accuracy: 0.9387\n","Epoch 6/30\n","499/500 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9388\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","500/500 [==============================] - 10s 21ms/step - loss: 0.1797 - accuracy: 0.9388 - val_loss: 0.1820 - val_accuracy: 0.9345\n","Epoch 7/30\n","500/500 [==============================] - 10s 21ms/step - loss: 0.1626 - accuracy: 0.9438 - val_loss: 0.1632 - val_accuracy: 0.9428\n","Epoch 8/30\n","499/500 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9458\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n","500/500 [==============================] - 10s 21ms/step - loss: 0.1568 - accuracy: 0.9457 - val_loss: 0.1636 - val_accuracy: 0.9435\n","Epoch 9/30\n","500/500 [==============================] - 10s 21ms/step - loss: 0.1562 - accuracy: 0.9467 - val_loss: 0.1628 - val_accuracy: 0.9437\n","Epoch 10/30\n","500/500 [==============================] - 10s 21ms/step - loss: 0.1511 - accuracy: 0.9487 - val_loss: 0.1620 - val_accuracy: 0.9444\n","Epoch 11/30\n","499/500 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.9491\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n","500/500 [==============================] - 10s 21ms/step - loss: 0.1503 - accuracy: 0.9491 - val_loss: 0.1622 - val_accuracy: 0.9444\n","Epoch 12/30\n","499/500 [============================>.] - ETA: 0s - loss: 0.1502 - accuracy: 0.9480\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n","500/500 [==============================] - 10s 21ms/step - loss: 0.1503 - accuracy: 0.9480 - val_loss: 0.1623 - val_accuracy: 0.9446\n","Epoch 00012: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7ivfufH2bUZ_"},"source":["#### Evaluate the model\n","The model is evaluated by analyzing the curves of loss and accuracy in both training and validation. \n","\n","Ideally, the curves of training and validation should be more or less identical in both accuracy and loss towards the last epochs, in order to obtain a generalized model.\n","\n","However, there can be cases where the validation loss is consistently greater than the training loss. This indicates underfitting. \n","\n","On the contrary, if the validation loss follows the training loss and then starts to deviate by increasing, we are overfitting.\n","\n","There could also be cases where the validation loss is consistently lesser than the training loss. There are three main reasons for this:\n","1. Regularization is applied during training, but not during validation.\n","2. Training loss is computed during each epoch, while validation loss is computed after each epoch. \n","3. The validation set is \"easier\" than the training set."]},{"cell_type":"code","metadata":{"id":"VDSv1whEXRhn","colab":{"base_uri":"https://localhost:8080/","height":573},"executionInfo":{"status":"ok","timestamp":1606423371384,"user_tz":-60,"elapsed":143087,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"c70417cd-9cee-4330-d7b2-bef58285da64"},"source":["# analyze training and validation accuracy/loss\n","plot_performance(history)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"GiZtEQgfHyfC"},"source":["#### Test the modified model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXlv1e6nbfO1","executionInfo":{"status":"ok","timestamp":1606423373567,"user_tz":-60,"elapsed":145264,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"009a8c22-7d2b-488c-ef46-b5fbb3f14272"},"source":["# test the model and print out the results\n","test_loss, test_acc = xcep_mod.evaluate(test_images, test_labels, verbose=0)\n","print(\"Test loss: {0:.{1}}\".format(test_loss, 4))\n","print(\"Test accuracy: {0:.{1}}\".format(test_acc, 4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test loss: 0.1818\n","Test accuracy: 0.94\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n-gR8XOD2oLk"},"source":["#### Model conclusion\n","Form the accuracy and loss plots, it can be seen that validation and accuracy loss differs only slightly from those in the training towards the last epochs. Even though there is a deviation, the validation loss did not start to increase which indicates that the fit can be considered good, nevertheless.\n","\n","The test accuracy is \\~94% which is highest achieved among all produced models so far. The test loss of \\~0.182 is the lowest we have obtained compared to previous models."]},{"cell_type":"markdown","metadata":{"id":"QrNpmxzTHozL"},"source":["#### Discussion\n","Accuracywise, this is the best model we have obtained compared to all previously made models. Compared to the modified VGG16 this model had:\n","- 103,595,810 parameters of which 948,936 were nontrainable. This is more than twice as many as in the VGG16.\n","- a larger batch size of 100, compared to 50 in VGG16.\n","- the Adam optimizer with a learning rate of 0.001, i.e., the same as in VGG16.\n","- the same callbacks in terms of early stopping and reduced learning rate.\n","- the same set of additional layers as in VGG16.\n","- a higher accuracy of 94%, compared to \\~92.7% in VGG16.\n","\n","Among the hyperparameters and the \"custom\" dense layers, it is only the batch size that differ between the modified Xception and VGG16. Hence, we can argue that the models are, more or less, comparable. On one hand, the underlying pretrained model in each case have been trained on the same ImageNet data. On the other hand, their corresponding architecture differ widely. We can also note that results probably would be different if we had incorporated more (or fewer) layers of the pretrained models.\n","\n","However, by the current modifications of the models, we have shown that Xception performs considerably better than VGG16. This can be due to the fact that Xception utilizes separable depthwise convolutions, compared to VGG16, that has \"ordinary\" convolutions. Another aspect is the regularization used in each model. Xception has batch normalization between each layer, while VGG16 has weight decay and dropout in the fully-connected layers (not considered in this laboration). \n","\n","With respect to these factors (among other structural differences), there is support that the Xception architecture is better for the Fashion MNIST classification problem. Although Xception is favorable with the modifications made in this laboration, an interesting extension of the comparison would be to utilize the full pretrained models. As well as letting some of the pretrained layers in Xception be trainable, as we did with VGG16 in part B. \n","\n","Lastly, we can distinguish between the modified Xception and the previous models from laboration 1 and 2. The main difference is the size and depth of models. I believe that my initial models were too small to get above 93% accuracy. Besides that, my initial models had an architecture inspired by VGG16, which, as shown here, does not perform as well as Xception. Therefore, a structural change as well as an introduction of separable convolutions are to consider in further development of my previous Fashion MNIST classifiers. \n","\n"]}]}