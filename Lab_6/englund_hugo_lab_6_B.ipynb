{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"englund_hugo_lab_6_B.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMtEiP0Qy0y/bWAT+cWAbSc"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1dHg9_DZdJVM"},"source":["### Hugo Englund | 2020-12-19"]},{"cell_type":"markdown","metadata":{"id":"ZAtyn3G3dJMd"},"source":["# Laboration 6: Part B\r\n","### Outline\r\n","In this part of laboration we will implement a suitable performance measure for Q-learning, and then compare the best model from Part A with GÃ©rons various Deep Q-learning neural networks (DQN):\r\n","1. DQN \r\n","2. Double DQN\r\n","3. Dueling Double DQN"]},{"cell_type":"markdown","metadata":{"id":"U2sAgW--f-Us"},"source":["## Set up Colab environment"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uoCwJQV5dFtY","executionInfo":{"status":"ok","timestamp":1608390349450,"user_tz":-60,"elapsed":732,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"eb4b9fdf-51ee-4616-c567-5d6b3923b210"},"source":["# Import needed libraries\r\n","import tensorflow as tf\r\n","print('TensorFlow version:', tf.__version__)\r\n","\r\n","import tensorflow.keras\r\n","import tensorflow.keras as keras\r\n","from tensorflow.keras.models import Sequential\r\n","from tensorflow.keras.layers import Dense, LSTM, GRU\r\n","from tensorflow.keras.layers import Conv1D\r\n","from tensorflow.keras.layers import Dropout, BatchNormalization\r\n","from tensorflow.keras.layers import Flatten\r\n","from tensorflow.keras.utils import to_categorical\r\n","print('Keras version:',tensorflow.keras.__version__)\r\n","\r\n","# Helper libraries\r\n","import os\r\n","import numpy as np\r\n","import pandas as pd\r\n","import cv2\r\n","import sklearn\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn import metrics\r\n","from sklearn.preprocessing import MinMaxScaler\r\n","from numpy import argmax\r\n","\r\n","# Matlab plotting\r\n","import matplotlib\r\n","import matplotlib as mpl\r\n","import matplotlib.pyplot as plt\r\n","# To plot pretty figures\r\n","mpl.rc('axes', labelsize=14)\r\n","mpl.rc('xtick', labelsize=12)\r\n","mpl.rc('ytick', labelsize=12)\r\n","\r\n","# To get smooth animations\r\n","import matplotlib.animation as animation\r\n","mpl.rc('animation', html='jshtml')\r\n","# For this Lab\r\n","import random\r\n","import gym"],"execution_count":3,"outputs":[{"output_type":"stream","text":["TensorFlow version: 2.4.0\n","Keras version: 2.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsyf4YDagEYq","executionInfo":{"status":"ok","timestamp":1608390349792,"user_tz":-60,"elapsed":1061,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"52a639c4-3a4c-407a-ec8d-b4b1cf7b9afa"},"source":["# Test for GPU and determine what GPU we have\r\n","# Could also use https://github.com/anderskm/gputil\r\n","import sys\r\n","if not tf.config.list_physical_devices('GPU'):\r\n","    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\r\n","    IN_COLAB = 'google.colab' in sys.modules\r\n","    if IN_COLAB:\r\n","        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\r\n","else:\r\n","    !nvidia-smi -L"],"execution_count":4,"outputs":[{"output_type":"stream","text":["No GPU was detected. CNNs can be very slow without a GPU.\n","Go to Runtime > Change runtime and select a GPU hardware accelerator.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fXUyM3QngQUk"},"source":["## Create environment"]},{"cell_type":"code","metadata":{"id":"MEBA6j1igLUf","executionInfo":{"status":"ok","timestamp":1608390349793,"user_tz":-60,"elapsed":1060,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["# Define some useful functions\r\n","def running_mean(x, N=10):\r\n","    \"\"\" Return the running mean of N element in a list\r\n","    \"\"\"\r\n","    cumsum = np.cumsum(np.insert(x, 0, 0)) \r\n","    return (cumsum[N:] - cumsum[:-N]) / float(N)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILqq9aDDgTyt","executionInfo":{"status":"ok","timestamp":1608390349793,"user_tz":-60,"elapsed":1058,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"f6917943-ad2f-4119-8509-9f033981f8ff"},"source":["# Create the Gym environment (CartPole)\r\n","env = gym.make('CartPole-v1')\r\n","\r\n","print('Action space is:', env.action_space)\r\n","print('Observation space is:', env.observation_space)\r\n","\r\n","# Let's initialize the environment by calling is reset() method. This returns an observation:\r\n","env.seed(42)\r\n","obs = env.reset()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Action space is: Discrete(2)\n","Observation space is: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kRRkjmoCgcUb"},"source":["## Define functions\r\n","Define some methods for training our agent, plotting etc.\r\n"]},{"cell_type":"code","metadata":{"id":"496pKDlXgdiD","executionInfo":{"status":"ok","timestamp":1608390349794,"user_tz":-60,"elapsed":1057,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["# Map a 4-dimensional state into a state index\r\n","# The state space is the four dimensions: x, x_dot, theta, theta_dot\r\n","def make_state(observation, DISCRETE_STEPS):\r\n","    \"\"\" Map a 4-dimensional state into a state index\r\n","    \"\"\"\r\n","    low = [-4.8, -10., -0.41888, -10.] # ( changed -0.41 into more correct -0.41888 for 24 deg.)\r\n","    high = [4.8, 10., 0.41888, 10.]\r\n","    state = 0\r\n","\r\n","    for i in range(4):\r\n","        # State variable, projected to the [0, 1] range\r\n","        state_variable = (observation[i] - low[i]) / (high[i] - low[i])\r\n","\r\n","        # Discretize. A variable having a value of 0.53 will lead to the integer 5,\r\n","        # for instance.\r\n","        state_discrete = int(state_variable * DISCRETE_STEPS)\r\n","        state_discrete = max(0, state_discrete) # should not be needed\r\n","        state_discrete = min(DISCRETE_STEPS-1, state_discrete)\r\n","\r\n","        state *= DISCRETE_STEPS\r\n","        state += state_discrete\r\n","        # Make state into a 4 \"digit\" number (between 0 and 9999, if 10 discrete steps)\r\n","    return state"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OV5inU19gfS7","executionInfo":{"status":"ok","timestamp":1608390349794,"user_tz":-60,"elapsed":1056,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["def plot_reward(episode_reward):\r\n","    '''\r\n","        plots episode reward along with\r\n","        the running episode mean\r\n","    '''\r\n","    plt.figure(figsize=(16, 4))\r\n","    plt.plot(episode_reward,\"b\")\r\n","    y_av = running_mean(episode_reward, N=100)\r\n","    plt.plot(y_av,\"r\")\r\n","    plt.show()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"oNbLh0kFgkM2","executionInfo":{"status":"ok","timestamp":1608390349795,"user_tz":-60,"elapsed":1055,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["def get_frames(qtable):\r\n","    '''\r\n","        builds the frame for animation\r\n","    '''\r\n","\r\n","    frames = []\r\n","    # Animate a run with current qtable\r\n","    # env.seed(42)\r\n","\r\n","    # Reset cart and get an initial state\r\n","    state = make_state(env.reset(), 12) \r\n","\r\n","    for step in range(200):\r\n","        # From the state find an action\r\n","        qvalues = qtable[state]\r\n","        action = argmax(qvalues) # greedy action\r\n","        # Perform an action to the environment\r\n","        next_state, reward, terminate, info = env.step(action)\r\n","        state = make_state(next_state, 12)\r\n","\r\n","        if terminate:\r\n","            break\r\n","        img = env.render(mode=\"rgb_array\")\r\n","        frames.append(img)\r\n","\r\n","    return frames"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"gLvVR9Zmgn2D","executionInfo":{"status":"ok","timestamp":1608396508457,"user_tz":-60,"elapsed":760,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["def train_agent_decay(EPISODES, EPSILON, GAMMA, LEARNING_RATE, DISCRETE_STEPS, ALPHA_DECAY, PRINT):\r\n","    '''\r\n","        trains the agent for the given parameters\r\n","        and returns the Q-table and the episode rewards\r\n","    '''\r\n","\r\n","    # Initialise statistics to zero\r\n","    average_cumulative_reward = 0.0\r\n","    episode_reward = np.zeros(EPISODES)\r\n","\r\n","    # Q-table for the discretized states, and two actions\r\n","    num_states = DISCRETE_STEPS ** 4\r\n","    qtable = [[0., 0.] for state in range(num_states)]\r\n","    print('Q-table = %.0f x %.0f' % (len(qtable),len(qtable[0]) ))\r\n","\r\n","    # Decay settings\r\n","    MIN_EPSILON = 0.01\r\n","    MIN_LR = 0.01\r\n","\r\n","    # Loop over episodes\r\n","    for i in range(EPISODES):\r\n","        state4D = env.reset()\r\n","        state = make_state(state4D, DISCRETE_STEPS)\r\n","\r\n","        terminate = False\r\n","        cumulative_reward = 0.0\r\n","  \r\n","        # decrease epsilon\r\n","        EPSILON = max(EPSILON * 0.9995, MIN_EPSILON)\r\n","\r\n","        # decrease learning rate\r\n","        alpha = max(LEARNING_RATE / (1 + i * ALPHA_DECAY), MIN_LR)\r\n","\r\n","        for _ in range(200):\r\n","            # Compute what the greedy action for the current state is\r\n","            qvalues = qtable[state]\r\n","            greedy_action = argmax(qvalues)\r\n","\r\n","            # Sometimes, the agent takes a random action, to explore the environment\r\n","            if random.random() < EPSILON:\r\n","                action = random.randrange(2)\r\n","            else:\r\n","                action = greedy_action\r\n","\r\n","            # Perform the action\r\n","            obs, reward, terminate, info = env.step(action)  # info is ignored\r\n","            next_state = make_state(obs, DISCRETE_STEPS)\r\n","\r\n","            if terminate:\r\n","                break\r\n","\r\n","            # Update the Q-Table\r\n","            td_error = reward + GAMMA * max(qtable[next_state]) - qtable[state][action]\r\n","            qtable[state][action] += alpha * td_error\r\n","\r\n","            # Update statistics\r\n","            cumulative_reward += reward\r\n","            state = next_state\r\n","\r\n","        # store reward for every episode\r\n","        episode_reward[i] = cumulative_reward\r\n","        \r\n","        # Per-episode statistics\r\n","        if ((i % 500) == 0 and PRINT):\r\n","            print(i, cumulative_reward, sep=',')\r\n","\r\n","    return qtable, episode_reward"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQr8Jq9yh5Vz"},"source":["## Set random seed\r\n","We define a random seed so we can reproduce our results:"]},{"cell_type":"code","metadata":{"id":"NqU-1Etfh460","executionInfo":{"status":"ok","timestamp":1608396560401,"user_tz":-60,"elapsed":689,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["env.seed(7)\r\n","np.random.seed(7)\r\n","tf.random.set_seed(7)"],"execution_count":89,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBm6vR7kg3rT"},"source":["## Implement performance measure\r\n","In order to evaluate the model in a better way we can make use of the fact that a total episode reward of 200 is considered as a \"completed epsiode\". Given that, we can compute the average error, i.e. subtract the episode reward from 200 for each episode, for each model, respectively.\r\n","\r\n","By this performance metric we get a feeling for how well the agent is performing in terms of solving the cartpole problem, as well as a fair measure when comparing models to each other. "]},{"cell_type":"code","metadata":{"id":"2t3Jvf2ZxuZl","executionInfo":{"status":"ok","timestamp":1608396560700,"user_tz":-60,"elapsed":974,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["def mean_error(reward):\r\n","    '''\r\n","        computes the average error\r\n","        over all episodes\r\n","    '''\r\n","    return np.mean(200 - np.array(reward))"],"execution_count":90,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95m8MOulgpDM"},"source":["## Implement model from Part A\r\n","First, we implement our model from Part A but with only 600 episodes and 200 steps per epsiode for it to be comparable with GÃ©ron's models and for our performance measure to make sense:"]},{"cell_type":"code","metadata":{"id":"1mp2ZbAkgvlG","executionInfo":{"status":"ok","timestamp":1608396561073,"user_tz":-60,"elapsed":1340,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["# hyperparameters\r\n","EPISODES       = 600    # Number of eposides to run, org. val = 15000\r\n","EPSILON        = 0.8    # Chance to explore (take a radom step instead of an \"optimal\" one), org val = 0.1\r\n","GAMMA          = 0.9    # How much previous steps should be rewarded now, discount rate, org val = 0.9\r\n","LEARNING_RATE  = 0.5    # Learning rate, org val = 0.1\r\n","DISCRETE_STEPS = 12     # Discretization steps per state variable (aviod odd numbers), org val = 10\r\n","ALPHA_DECAY    = 0.001  # Learning rate decay\r\n","PRINT          = True   # Determine if cumulative episode rewards should be printed"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PWgJCcYEgwW0","executionInfo":{"status":"ok","timestamp":1608396561388,"user_tz":-60,"elapsed":1649,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"19f7eb78-73ef-4991-b20c-cb493131417e"},"source":["q, r = train_agent_decay(EPISODES, EPSILON, GAMMA, LEARNING_RATE, DISCRETE_STEPS, ALPHA_DECAY, PRINT)"],"execution_count":92,"outputs":[{"output_type":"stream","text":["Q-table = 20736 x 2\n","0,14.0\n","500,12.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"euTU2xOg4Hm-","executionInfo":{"status":"ok","timestamp":1608396561388,"user_tz":-60,"elapsed":1640,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["# save results in dict\r\n","results = dict()"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KdJgNP3r4Hw","executionInfo":{"status":"ok","timestamp":1608396561389,"user_tz":-60,"elapsed":1635,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["# save mean error for my model\r\n","results['My model'] = mean_error(r)"],"execution_count":94,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RPoHaq5shH0c"},"source":["## Implement GÃ©ron's DQN models\r\n","Second, we implement GÃ©ron's DQN models. All code provided from GÃ©ron's book."]},{"cell_type":"markdown","metadata":{"id":"Dbd9wwmUhsKz"},"source":["### DQN\r\n","Define helper methods:"]},{"cell_type":"code","metadata":{"id":"EcHalsgkhaUb","executionInfo":{"status":"ok","timestamp":1608396561389,"user_tz":-60,"elapsed":1627,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["keras.backend.clear_session()\r\n","tf.random.set_seed(7)\r\n","np.random.seed(7)\r\n","\r\n","env = gym.make(\"CartPole-v1\")\r\n","input_shape = [4] # == env.observation_space.shape\r\n","n_outputs = 2 # == env.action_space.n\r\n","\r\n","model = keras.models.Sequential([\r\n","    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\r\n","    keras.layers.Dense(32, activation=\"elu\"),\r\n","    keras.layers.Dense(n_outputs)\r\n","])"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"id":"6kEDtxVJhZkK","executionInfo":{"status":"ok","timestamp":1608396561389,"user_tz":-60,"elapsed":1621,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["def epsilon_greedy_policy(state, epsilon=0):\r\n","    if np.random.rand() < epsilon:\r\n","        return np.random.randint(2)\r\n","    else:\r\n","        Q_values = model.predict(state[np.newaxis])\r\n","        return np.argmax(Q_values[0])"],"execution_count":96,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_rhnEGzhHHG","executionInfo":{"status":"ok","timestamp":1608396561826,"user_tz":-60,"elapsed":2052,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["from collections import deque\r\n","\r\n","replay_memory = deque(maxlen=2000)"],"execution_count":97,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xcvd5xYhjkI","executionInfo":{"status":"ok","timestamp":1608396561827,"user_tz":-60,"elapsed":2047,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["def sample_experiences(batch_size):\r\n","    indices = np.random.randint(len(replay_memory), size=batch_size)\r\n","    batch = [replay_memory[index] for index in indices]\r\n","    states, actions, rewards, next_states, dones = [\r\n","        np.array([experience[field_index] for experience in batch])\r\n","        for field_index in range(5)]\r\n","    return states, actions, rewards, next_states, dones"],"execution_count":98,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vkem_O7hlfj","executionInfo":{"status":"ok","timestamp":1608396561827,"user_tz":-60,"elapsed":2041,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["def play_one_step(env, state, epsilon):\r\n","    action = epsilon_greedy_policy(state, epsilon)\r\n","    next_state, reward, done, info = env.step(action)\r\n","    replay_memory.append((state, action, reward, next_state, done))\r\n","    return next_state, reward, done, info"],"execution_count":99,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6mD47ukhqXr","executionInfo":{"status":"ok","timestamp":1608396561827,"user_tz":-60,"elapsed":2035,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["batch_size = 32\r\n","discount_rate = 0.95\r\n","optimizer = keras.optimizers.Adam(lr=1e-3)\r\n","loss_fn = keras.losses.mean_squared_error\r\n","\r\n","def training_step(batch_size):\r\n","    experiences = sample_experiences(batch_size)\r\n","    states, actions, rewards, next_states, dones = experiences\r\n","    next_Q_values = model.predict(next_states)\r\n","    max_next_Q_values = np.max(next_Q_values, axis=1)\r\n","    target_Q_values = (rewards +\r\n","                       (1 - dones) * discount_rate * max_next_Q_values)\r\n","    target_Q_values = target_Q_values.reshape(-1, 1)\r\n","    mask = tf.one_hot(actions, n_outputs)\r\n","    with tf.GradientTape() as tape:\r\n","        all_Q_values = model(states)\r\n","        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\r\n","        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\r\n","    grads = tape.gradient(loss, model.trainable_variables)\r\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))"],"execution_count":100,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BWue2nYh0ml","executionInfo":{"status":"ok","timestamp":1608396843175,"user_tz":-60,"elapsed":283376,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"cba80898-6b1c-4ea9-e750-7ac455dc25f0"},"source":["rewards = [] \r\n","best_score = 0\r\n","\r\n","for episode in range(600):\r\n","    obs = env.reset()    \r\n","    for step in range(200):\r\n","        epsilon = max(1 - episode / 500, 0.01)\r\n","        obs, reward, done, info = play_one_step(env, obs, epsilon)\r\n","        if done:\r\n","            break\r\n","    rewards.append(step) # Not shown in the book\r\n","    if step > best_score: # Not shown\r\n","        best_weights = model.get_weights() # Not shown\r\n","        best_score = step # Not shown\r\n","    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\r\n","    if episode > 50:\r\n","        training_step(batch_size)\r\n","\r\n","model.set_weights(best_weights)"],"execution_count":101,"outputs":[{"output_type":"stream","text":["Episode: 599, Steps: 13, eps: 0.010"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RJ-YyYAlmAzE","executionInfo":{"status":"ok","timestamp":1608396843176,"user_tz":-60,"elapsed":283367,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["# save mean error for DQN\r\n","results['DQN'] = mean_error(rewards)"],"execution_count":102,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VSlsxdzxkON7"},"source":["### Double DQN"]},{"cell_type":"code","metadata":{"id":"MhwutqGIkN7T","executionInfo":{"status":"ok","timestamp":1608396843176,"user_tz":-60,"elapsed":283360,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["keras.backend.clear_session()\r\n","tf.random.set_seed(7)\r\n","np.random.seed(7)\r\n","\r\n","model = keras.models.Sequential([\r\n","    keras.layers.Dense(32, activation=\"elu\", input_shape=[4]),\r\n","    keras.layers.Dense(32, activation=\"elu\"),\r\n","    keras.layers.Dense(n_outputs)\r\n","])\r\n","\r\n","target = keras.models.clone_model(model)\r\n","target.set_weights(model.get_weights())"],"execution_count":103,"outputs":[]},{"cell_type":"code","metadata":{"id":"YwjO6RmfkWRN","executionInfo":{"status":"ok","timestamp":1608396843176,"user_tz":-60,"elapsed":283354,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["batch_size = 32\r\n","discount_rate = 0.95\r\n","optimizer = keras.optimizers.Adam(lr=1e-3)\r\n","loss_fn = keras.losses.Huber()\r\n","\r\n","def training_step(batch_size):\r\n","    experiences = sample_experiences(batch_size)\r\n","    states, actions, rewards, next_states, dones = experiences\r\n","    next_Q_values = model.predict(next_states)\r\n","    best_next_actions = np.argmax(next_Q_values, axis=1)\r\n","    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\r\n","    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\r\n","    target_Q_values = (rewards + \r\n","                       (1 - dones) * discount_rate * next_best_Q_values)\r\n","    target_Q_values = target_Q_values.reshape(-1, 1)\r\n","    mask = tf.one_hot(actions, n_outputs)\r\n","    with tf.GradientTape() as tape:\r\n","        all_Q_values = model(states)\r\n","        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\r\n","        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\r\n","    grads = tape.gradient(loss, model.trainable_variables)\r\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))"],"execution_count":104,"outputs":[]},{"cell_type":"code","metadata":{"id":"I3WfANU5kW9i","executionInfo":{"status":"ok","timestamp":1608396843177,"user_tz":-60,"elapsed":283348,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["replay_memory = deque(maxlen=2000)"],"execution_count":105,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yaYb2FMckYzi","executionInfo":{"status":"ok","timestamp":1608397240186,"user_tz":-60,"elapsed":680351,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"923e9fa2-3de3-430b-fc52-cde4c1e81441"},"source":["rewards = []\r\n","best_score = 0\r\n","\r\n","for episode in range(600):\r\n","    obs = env.reset()    \r\n","    for step in range(200):\r\n","        epsilon = max(1 - episode / 500, 0.01)\r\n","        obs, reward, done, info = play_one_step(env, obs, epsilon)\r\n","        if done:\r\n","            break\r\n","    rewards.append(step)\r\n","    if step > best_score:\r\n","        best_weights = model.get_weights()\r\n","        best_score = step\r\n","    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\r\n","    if episode > 50:\r\n","        training_step(batch_size)\r\n","    if episode % 50 == 0:\r\n","        target.set_weights(model.get_weights())\r\n","\r\n","model.set_weights(best_weights)"],"execution_count":106,"outputs":[{"output_type":"stream","text":["Episode: 599, Steps: 11, eps: 0.010"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"isT6kBYr4pX3","executionInfo":{"status":"ok","timestamp":1608397240188,"user_tz":-60,"elapsed":680344,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["# save mean error for Double DQN\r\n","results['Double DQN'] = mean_error(rewards)"],"execution_count":107,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YuopmKWkd_z"},"source":["### Dueling Double DQN"]},{"cell_type":"code","metadata":{"id":"FqW2vd67kg0z","executionInfo":{"status":"ok","timestamp":1608397240190,"user_tz":-60,"elapsed":680340,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["keras.backend.clear_session()\r\n","tf.random.set_seed(7)\r\n","np.random.seed(7)\r\n","\r\n","K = keras.backend\r\n","input_states = keras.layers.Input(shape=[4])\r\n","hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\r\n","hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\r\n","state_values = keras.layers.Dense(1)(hidden2)\r\n","raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\r\n","advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\r\n","Q_values = state_values + advantages\r\n","model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\r\n","\r\n","target = keras.models.clone_model(model)\r\n","target.set_weights(model.get_weights())"],"execution_count":108,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-eJJB52kjqT","executionInfo":{"status":"ok","timestamp":1608397240191,"user_tz":-60,"elapsed":680334,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["batch_size = 32\r\n","discount_rate = 0.95\r\n","optimizer = keras.optimizers.Adam(lr=1e-2)\r\n","loss_fn = keras.losses.Huber()\r\n","\r\n","def training_step(batch_size):\r\n","    experiences = sample_experiences(batch_size)\r\n","    states, actions, rewards, next_states, dones = experiences\r\n","    next_Q_values = model.predict(next_states)\r\n","    best_next_actions = np.argmax(next_Q_values, axis=1)\r\n","    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\r\n","    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\r\n","    target_Q_values = (rewards + \r\n","                       (1 - dones) * discount_rate * next_best_Q_values)\r\n","    target_Q_values = target_Q_values.reshape(-1, 1)\r\n","    mask = tf.one_hot(actions, n_outputs)\r\n","    with tf.GradientTape() as tape:\r\n","        all_Q_values = model(states)\r\n","        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\r\n","        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\r\n","    grads = tape.gradient(loss, model.trainable_variables)\r\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))"],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"id":"FnDjWlPgknAf","executionInfo":{"status":"ok","timestamp":1608397240192,"user_tz":-60,"elapsed":680329,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["replay_memory = deque(maxlen=2000)"],"execution_count":110,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdLp4wRtkpGM","executionInfo":{"status":"ok","timestamp":1608397578661,"user_tz":-60,"elapsed":1018791,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"8ee4ce37-6aad-4ca5-f9d4-2fd3bb523691"},"source":["rewards = []\r\n","best_score = 0\r\n","\r\n","for episode in range(600):\r\n","    obs = env.reset()    \r\n","    for step in range(200):\r\n","        epsilon = max(1 - episode / 500, 0.01)\r\n","        obs, reward, done, info = play_one_step(env, obs, epsilon)\r\n","        if done:\r\n","            break\r\n","    rewards.append(step)\r\n","    if step > best_score:\r\n","        best_weights = model.get_weights()\r\n","        best_score = step\r\n","    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\r\n","    if episode > 50:\r\n","        training_step(batch_size)\r\n","    if episode % 200 == 0:\r\n","        target.set_weights(model.get_weights())\r\n","\r\n","model.set_weights(best_weights)"],"execution_count":111,"outputs":[{"output_type":"stream","text":["Episode: 599, Steps: 120, eps: 0.010"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sgEhydukg6hi","executionInfo":{"status":"ok","timestamp":1608397578662,"user_tz":-60,"elapsed":1018785,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}}},"source":["# save mean error for Dueling Double DQN\r\n","results['Dueling Double DQN'] = mean_error(rewards)"],"execution_count":112,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k6LVfK4146ae"},"source":["## Model comparison"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsEUoisy46Cm","executionInfo":{"status":"ok","timestamp":1608398539424,"user_tz":-60,"elapsed":730,"user":{"displayName":"Hugo Englund","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJLziSR4xpl7D56Ur4HL3y_6us4Ugp_gh0pRP1=s64","userId":"13908934327195697226"}},"outputId":"916fd33a-bbc8-44da-fa3f-f2a2930bb5b8"},"source":["# print out result for each model\r\n","print('Mean error')\r\n","print('---------------')\r\n","for mdl, ME in results.items():\r\n","    print(mdl + ': ' + str(round(ME, 3)))"],"execution_count":119,"outputs":[{"output_type":"stream","text":["Mean error\n","---------------\n","My model: 177.093\n","DQN: 178.205\n","Double DQN: 170.807\n","Dueling Double DQN: 177.657\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zhGNre2A5tpG"},"source":["From the mean errors for each model, we can see that the Double DQN performed best, while my model, DQN and Dueling Double DQN performed similarly. We can conclude that we obtained a decent model in Part A, which is clearly comparable with the deep Q-learning models. However, we must note that my model is carefully tuned while the DQN models have not been tuned any further than what GÃ©ron perhaps have done. Therefore, it is likely that the deep variations of Q-learning, with a bit of fine-tuning, would outperform my model easily since the capacity of the deep models is higher than in my \"simple\" implementation. \r\n","\r\n","Although my model is not the best, the variation in mean error is quite small so from a broader perspective we cannot really distungish between the models. On the other hand, if we consider the lack of tuning in the DQN models we can safely dismiss my model and rely on DQN for further improvements in the cartpole problem.\r\n","_____"]}]}